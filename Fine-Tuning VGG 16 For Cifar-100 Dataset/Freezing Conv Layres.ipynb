{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":1782442,"sourceType":"datasetVersion","datasetId":1059701}],"dockerImageVersionId":30685,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-12T17:50:47.186834Z","iopub.execute_input":"2024-04-12T17:50:47.187341Z","iopub.status.idle":"2024-04-12T17:50:47.554561Z","shell.execute_reply.started":"2024-04-12T17:50:47.187298Z","shell.execute_reply":"2024-04-12T17:50:47.553606Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/meta\n/kaggle/input/file.txt\n/kaggle/input/test\n/kaggle/input/train\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Transforming data","metadata":{}},{"cell_type":"code","source":"import torch \nimport torchvision\n\n\ntransform_train = torchvision.transforms.Compose([\n    torchvision.transforms.Resize(224),  # Resize to 256x256\n    #torchvision.transforms.CenterCrop(64),  # Crop a central 224x224 region\n    #torchvision.transforms.RandomResizedCrop(size=(64,64), scale=(0.8, 1.0)),  # Randomly resize and crop\n    torchvision.transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip horizontally\n    torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    torchvision.transforms.ToTensor(),  # Convert to PyTorch tensor\n    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for ImageNet\n])\n\ntransform_test=torchvision.transforms.Compose([\n    torchvision.transforms.Resize(224),  \n    torchvision.transforms.ToTensor(),  # Convert to PyTorch tensor\n    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2024-04-12T17:50:47.556594Z","iopub.execute_input":"2024-04-12T17:50:47.557499Z","iopub.status.idle":"2024-04-12T17:50:50.826312Z","shell.execute_reply.started":"2024-04-12T17:50:47.557461Z","shell.execute_reply":"2024-04-12T17:50:50.825490Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Importing and Loading Data ","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom torchvision.datasets import CIFAR100\n\nroot=\"./data\"\ntrain = CIFAR100(root=root, transform=transform_train,download=True,train=True)\ntest= CIFAR100(root=root, transform=transform_test,download=True,train=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T17:50:50.827354Z","iopub.execute_input":"2024-04-12T17:50:50.827755Z","iopub.status.idle":"2024-04-12T17:50:52.640538Z","shell.execute_reply.started":"2024-04-12T17:50:50.827729Z","shell.execute_reply":"2024-04-12T17:50:52.639655Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Initialize Dataloader","metadata":{}},{"cell_type":"code","source":"train_loader = DataLoader(train, batch_size=100, shuffle=True)\ntest_loader= DataLoader(test, batch_size=16, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T17:50:52.642504Z","iopub.execute_input":"2024-04-12T17:50:52.642803Z","iopub.status.idle":"2024-04-12T17:50:52.647666Z","shell.execute_reply.started":"2024-04-12T17:50:52.642777Z","shell.execute_reply":"2024-04-12T17:50:52.646660Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"len(train_loader)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T17:50:52.648806Z","iopub.execute_input":"2024-04-12T17:50:52.649070Z","iopub.status.idle":"2024-04-12T17:50:52.661858Z","shell.execute_reply.started":"2024-04-12T17:50:52.649046Z","shell.execute_reply":"2024-04-12T17:50:52.660962Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"500"},"metadata":{}}]},{"cell_type":"markdown","source":"# Importing torch","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T17:50:52.662932Z","iopub.execute_input":"2024-04-12T17:50:52.663204Z","iopub.status.idle":"2024-04-12T17:50:52.677232Z","shell.execute_reply.started":"2024-04-12T17:50:52.663181Z","shell.execute_reply":"2024-04-12T17:50:52.676434Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'\\nclass VGG16(nn.Module):\\n    def __init__(self, num_classes=10):\\n        super(VGG16, self).__init__()\\n        self.layer1 = nn.Sequential(\\n            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\\n            nn.BatchNorm2d(64),\\n            nn.ReLU())\\n        self.layer2 = nn.Sequential(\\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\\n            nn.BatchNorm2d(64),\\n            nn.ReLU(), \\n            nn.MaxPool2d(kernel_size = 2, stride = 2))\\n        self.layer3 = nn.Sequential(\\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\\n            nn.BatchNorm2d(128),\\n            nn.ReLU())\\n        self.layer4 = nn.Sequential(\\n            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\\n            nn.BatchNorm2d(128),\\n            nn.ReLU(),\\n            nn.MaxPool2d(kernel_size = 2, stride = 2))\\n        self.layer5 = nn.Sequential(\\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\\n            nn.BatchNorm2d(256),\\n            nn.ReLU())\\n        self.layer6 = nn.Sequential(\\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\\n            nn.BatchNorm2d(256),\\n            nn.ReLU())\\n        self.layer7 = nn.Sequential(\\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\\n            nn.BatchNorm2d(256),\\n            nn.ReLU(),\\n            nn.MaxPool2d(kernel_size = 2, stride = 2))\\n        self.layer8 = nn.Sequential(\\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\\n            nn.BatchNorm2d(512),\\n            nn.ReLU())\\n        self.layer9 = nn.Sequential(\\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\\n            nn.BatchNorm2d(512),\\n            nn.ReLU())\\n        self.layer10 = nn.Sequential(\\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\\n            nn.BatchNorm2d(512),\\n            nn.ReLU(),\\n            nn.MaxPool2d(kernel_size = 2, stride = 2))\\n        self.layer11 = nn.Sequential(\\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\\n            nn.BatchNorm2d(512),\\n            nn.ReLU())\\n        self.layer12 = nn.Sequential(\\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\\n            nn.BatchNorm2d(512),\\n            nn.ReLU())\\n        self.layer13 = nn.Sequential(\\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\\n            nn.BatchNorm2d(512),\\n            nn.ReLU(),\\n            nn.MaxPool2d(kernel_size = 2, stride = 2))\\n            \\n        \\n        self.fc = nn.Sequential(\\n            nn.Dropout(0.5),\\n            nn.Linear(7*7*512, 4096),\\n            nn.ReLU())\\n        \\n        self.fc1 = nn.Sequential(\\n            nn.Dropout(0.5),\\n            nn.Linear(4096, 4096),\\n            nn.ReLU())\\n        self.fc2= nn.Sequential(\\n            nn.Linear(4096, num_classes))\\n        \\n    def forward(self, x):\\n        out = self.layer1(x)\\n        #print(out.shape)\\n        out = self.layer2(out)\\n        #print(out.shape)\\n        out = self.layer3(out)\\n        #print(out.shape)\\n        out = self.layer4(out)\\n        #print(out.shape)\\n        out = self.layer5(out)\\n        #print(out.shape)\\n        out = self.layer6(out)\\n        #print(out.shape)\\n        out = self.layer7(out)\\n        #print(out.shape)\\n        out = self.layer8(out)\\n        #print(out.shape)\\n        out = self.layer9(out)\\n        #print(out.shape)\\n        out = self.layer10(out)\\n        #print(out.shape)\\n        out = self.layer11(out)\\n        #print(out.shape)\\n        out = self.layer12(out)\\n        #print(out.shape)\\n        out = self.layer13(out)\\n        #print(out.shape)\\n    \\n        out=out.flatten(start_dim=1)\\n        #print(out.shape)\\n        out = self.fc(out)\\n        #print(out.shape)\\n        \\n        out = self.fc1(out)\\n        #print(out.shape)\\n        out = self.fc2(out)\\n        #print(out.shape)\\n        \\n        return out\\n        '"},"metadata":{}}]},{"cell_type":"markdown","source":"# Hyperparameter tuning and  Defining Model Architecture\n> Frezzing Conv Layers","metadata":{}},{"cell_type":"code","source":"import torchvision.models as models\ndevice='cuda'\nnum_classes = 100\nnum_epochs = 2\nlearning_rate = 0.001\nmodel = models.vgg16(weights='VGG16_Weights.DEFAULT')\n#model=nn.DataParallel(model)\n\n#Frezzing Conv Layers\nfor param in model.features.parameters():\n    param.requires_grad=False \n\n\n\n# Replace the last classifier layer\nnum_features_in = model.classifier[-1].in_features  # Get input size of the last layer\nmodel.classifier[-1] = torch.nn.Linear(num_features_in,100)\nmodel=model.to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.7)  \n\n\n# Train the model\ntotal_step = len(train_loader)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T17:50:52.678221Z","iopub.execute_input":"2024-04-12T17:50:52.678481Z","iopub.status.idle":"2024-04-12T17:50:54.601279Z","shell.execute_reply.started":"2024-04-12T17:50:52.678458Z","shell.execute_reply":"2024-04-12T17:50:54.600468Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model.features","metadata":{"execution":{"iopub.status.busy":"2024-04-12T17:50:54.602292Z","iopub.execute_input":"2024-04-12T17:50:54.602545Z","iopub.status.idle":"2024-04-12T17:50:54.608776Z","shell.execute_reply.started":"2024-04-12T17:50:54.602522Z","shell.execute_reply":"2024-04-12T17:50:54.607883Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Sequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU(inplace=True)\n  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU(inplace=True)\n  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (6): ReLU(inplace=True)\n  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (8): ReLU(inplace=True)\n  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (11): ReLU(inplace=True)\n  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (13): ReLU(inplace=True)\n  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (15): ReLU(inplace=True)\n  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (18): ReLU(inplace=True)\n  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (20): ReLU(inplace=True)\n  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (22): ReLU(inplace=True)\n  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (25): ReLU(inplace=True)\n  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (27): ReLU(inplace=True)\n  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (29): ReLU(inplace=True)\n  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"model.classifier","metadata":{"execution":{"iopub.status.busy":"2024-04-12T17:50:54.609906Z","iopub.execute_input":"2024-04-12T17:50:54.610198Z","iopub.status.idle":"2024-04-12T17:50:54.619447Z","shell.execute_reply.started":"2024-04-12T17:50:54.610174Z","shell.execute_reply":"2024-04-12T17:50:54.618542Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Sequential(\n  (0): Linear(in_features=25088, out_features=4096, bias=True)\n  (1): ReLU(inplace=True)\n  (2): Dropout(p=0.5, inplace=False)\n  (3): Linear(in_features=4096, out_features=4096, bias=True)\n  (4): ReLU(inplace=True)\n  (5): Dropout(p=0.5, inplace=False)\n  (6): Linear(in_features=4096, out_features=100, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"model.parameters","metadata":{"execution":{"iopub.status.busy":"2024-04-12T17:50:54.622209Z","iopub.execute_input":"2024-04-12T17:50:54.622490Z","iopub.status.idle":"2024-04-12T17:50:54.630952Z","shell.execute_reply.started":"2024-04-12T17:50:54.622459Z","shell.execute_reply":"2024-04-12T17:50:54.630127Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<bound method Module.parameters of VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=100, bias=True)\n  )\n)>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Traning and Testing loop","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\ntotal_step = len(train_loader)\npatience=2\npatience_count=0\nminimum_loss=float('inf')\n\nfor epoch in range(num_epochs):\n    \n    for  images, labels in tqdm(train_loader):  \n        correct = 0\n        total = 0\n        # Move tensors to the configured device\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        model.train()\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        \n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Loss {loss.item()}\" )\n    print('Accuracy of the network on the {}  images: {} %'.format(50000, 100 * correct / total))\n        \n    # Validation\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        eval_loss=0\n        for images, labels in tqdm(test_loader):\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            \n            eval_loss+=criterion(outputs, labels).item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            del images, labels, outputs\n        print(f\" Loss Minimum {minimum_loss} Loss_current {eval_loss}\")\n            \n        if eval_loss<=minimum_loss:\n            patience_count=0\n            minimum_loss=eval_loss\n        else:\n            patience_count+=1\n            print(f\"Patience Counter {patience_count}\")\n        print('Accuracy of the network on the {} validation images: {} %'.format(10000, 100 * correct / total))\n        \n        if patience_count>=patience:\n            print(f\"Early stopping initialized\")\n            break\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T17:50:54.632038Z","iopub.execute_input":"2024-04-12T17:50:54.632474Z","iopub.status.idle":"2024-04-12T18:07:39.174640Z","shell.execute_reply.started":"2024-04-12T17:50:54.632439Z","shell.execute_reply":"2024-04-12T18:07:39.173402Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"100%|██████████| 500/500 [07:41<00:00,  1.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss 2.7638161182403564\nAccuracy of the network on the 50000  images: 27.0 %\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 625/625 [00:38<00:00, 16.17it/s]\n","output_type":"stream"},{"name":"stdout","text":" Loss Minimum inf Loss_current 1591.2012491226196\nAccuracy of the network on the 10000 validation images: 35.64 %\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 500/500 [07:45<00:00,  1.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss 2.047910690307617\nAccuracy of the network on the 50000  images: 40.0 %\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 625/625 [00:38<00:00, 16.05it/s]","output_type":"stream"},{"name":"stdout","text":" Loss Minimum 1591.2012491226196 Loss_current 1254.19464802742\nAccuracy of the network on the 10000 validation images: 45.03 %\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Passing Radnom image to find Cnn output","metadata":{}},{"cell_type":"code","source":"model = model.to(device)\na=torch.randn(1,3,224,224)\na=a.to(device)\n#center_crop = torchvision.transforms.CenterCrop(64)\n\n# Apply the center cropping transform to the input image\n#a = center_crop(a)\n\nx=model(a)\nx.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-12T18:07:39.176287Z","iopub.execute_input":"2024-04-12T18:07:39.176781Z","iopub.status.idle":"2024-04-12T18:07:39.234228Z","shell.execute_reply.started":"2024-04-12T18:07:39.176725Z","shell.execute_reply":"2024-04-12T18:07:39.233247Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 100])"},"metadata":{}}]}]}