{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1782442,"sourceType":"datasetVersion","datasetId":1059701}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-11T17:47:01.184657Z","iopub.execute_input":"2024-07-11T17:47:01.185285Z","iopub.status.idle":"2024-07-11T17:47:01.200113Z","shell.execute_reply.started":"2024-07-11T17:47:01.185254Z","shell.execute_reply":"2024-07-11T17:47:01.199133Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/meta\n/kaggle/input/file.txt\n/kaggle/input/test\n/kaggle/input/train\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Transforming data","metadata":{}},{"cell_type":"code","source":"import torch \nimport torchvision\n\n\ntransform_train = torchvision.transforms.Compose([\n    torchvision.transforms.Resize(224),  # Resize to 256x256\n    #torchvision.transforms.CenterCrop(64),  # Crop a central 224x224 region\n    #torchvision.transforms.RandomResizedCrop(size=(64,64), scale=(0.8, 1.0)),  # Randomly resize and crop\n    torchvision.transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip horizontally\n    torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    torchvision.transforms.ToTensor(),  # Convert to PyTorch tensor\n    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for ImageNet\n])\n\ntransform_test=torchvision.transforms.Compose([\n    torchvision.transforms.Resize(224),  \n    torchvision.transforms.ToTensor(),  # Convert to PyTorch tensor\n    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2024-07-11T17:47:01.201958Z","iopub.execute_input":"2024-07-11T17:47:01.202225Z","iopub.status.idle":"2024-07-11T17:47:01.210207Z","shell.execute_reply.started":"2024-07-11T17:47:01.202202Z","shell.execute_reply":"2024-07-11T17:47:01.209213Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Importing and Loading Data ","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom torchvision.datasets import CIFAR100\n\nroot=\"./data\"\ntrain = CIFAR100(root=root, transform=transform_train,download=True,train=True)\ntest= CIFAR100(root=root, transform=transform_test,download=True,train=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T17:47:01.211618Z","iopub.execute_input":"2024-07-11T17:47:01.212262Z","iopub.status.idle":"2024-07-11T17:47:02.973222Z","shell.execute_reply.started":"2024-07-11T17:47:01.212230Z","shell.execute_reply":"2024-07-11T17:47:02.972298Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Initialize Dataloader","metadata":{}},{"cell_type":"code","source":"train_loader = DataLoader(train, batch_size=100, shuffle=True)\ntest_loader= DataLoader(test, batch_size=16, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T17:47:02.975925Z","iopub.execute_input":"2024-07-11T17:47:02.976525Z","iopub.status.idle":"2024-07-11T17:47:02.992252Z","shell.execute_reply.started":"2024-07-11T17:47:02.976491Z","shell.execute_reply":"2024-07-11T17:47:02.991375Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"len(train_loader)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T17:47:02.993400Z","iopub.execute_input":"2024-07-11T17:47:02.993741Z","iopub.status.idle":"2024-07-11T17:47:03.004481Z","shell.execute_reply.started":"2024-07-11T17:47:02.993711Z","shell.execute_reply":"2024-07-11T17:47:03.003605Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"500"},"metadata":{}}]},{"cell_type":"markdown","source":"# Importing torch","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2024-07-11T17:47:03.005686Z","iopub.execute_input":"2024-07-11T17:47:03.006098Z","iopub.status.idle":"2024-07-11T17:47:03.013215Z","shell.execute_reply.started":"2024-07-11T17:47:03.006069Z","shell.execute_reply":"2024-07-11T17:47:03.012388Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter tuning and  Defining Model Architecture\n## Freezing FC Layers","metadata":{}},{"cell_type":"code","source":"import torchvision.models as models\ndevice='cuda'\nnum_classes = 100\nnum_epochs = 1\nlearning_rate = 0.001\nmodel = models.vgg16(weights='VGG16_Weights.DEFAULT')\n#model=nn.DataParallel(model)\n\n#Frezzing FC Layers\nfor param in model.classifier.parameters():\n    param.requires_grad=False \n\n\n\n# Replace the last classifier layer\nnum_features_in = model.classifier[-1].in_features  # Get input size of the last layer\nmodel.classifier[-1] = torch.nn.Linear(num_features_in,100)\nmodel=model.to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.7)  \n\n\n# Train the model\ntotal_step = len(train_loader)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T17:47:03.014245Z","iopub.execute_input":"2024-07-11T17:47:03.014551Z","iopub.status.idle":"2024-07-11T17:47:08.147976Z","shell.execute_reply.started":"2024-07-11T17:47:03.014520Z","shell.execute_reply":"2024-07-11T17:47:08.146954Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:03<00:00, 171MB/s]  \n","output_type":"stream"}]},{"cell_type":"code","source":"model.features","metadata":{"execution":{"iopub.status.busy":"2024-07-11T17:47:08.149243Z","iopub.execute_input":"2024-07-11T17:47:08.149559Z","iopub.status.idle":"2024-07-11T17:47:08.157809Z","shell.execute_reply.started":"2024-07-11T17:47:08.149536Z","shell.execute_reply":"2024-07-11T17:47:08.156984Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Sequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU(inplace=True)\n  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU(inplace=True)\n  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (6): ReLU(inplace=True)\n  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (8): ReLU(inplace=True)\n  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (11): ReLU(inplace=True)\n  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (13): ReLU(inplace=True)\n  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (15): ReLU(inplace=True)\n  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (18): ReLU(inplace=True)\n  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (20): ReLU(inplace=True)\n  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (22): ReLU(inplace=True)\n  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (25): ReLU(inplace=True)\n  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (27): ReLU(inplace=True)\n  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (29): ReLU(inplace=True)\n  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"model.classifier","metadata":{"execution":{"iopub.status.busy":"2024-07-11T17:47:08.158889Z","iopub.execute_input":"2024-07-11T17:47:08.159150Z","iopub.status.idle":"2024-07-11T17:47:08.171818Z","shell.execute_reply.started":"2024-07-11T17:47:08.159129Z","shell.execute_reply":"2024-07-11T17:47:08.170799Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Sequential(\n  (0): Linear(in_features=25088, out_features=4096, bias=True)\n  (1): ReLU(inplace=True)\n  (2): Dropout(p=0.5, inplace=False)\n  (3): Linear(in_features=4096, out_features=4096, bias=True)\n  (4): ReLU(inplace=True)\n  (5): Dropout(p=0.5, inplace=False)\n  (6): Linear(in_features=4096, out_features=100, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"model.parameters","metadata":{"execution":{"iopub.status.busy":"2024-07-11T17:47:08.174875Z","iopub.execute_input":"2024-07-11T17:47:08.175222Z","iopub.status.idle":"2024-07-11T17:47:08.182993Z","shell.execute_reply.started":"2024-07-11T17:47:08.175194Z","shell.execute_reply":"2024-07-11T17:47:08.182103Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<bound method Module.parameters of VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=100, bias=True)\n  )\n)>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Traning and Testing loop","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\ntotal_step = len(train_loader)\npatience=2\npatience_count=0\nminimum_loss=float('inf')\n\nfor epoch in range(num_epochs):\n    \n    for  images, labels in tqdm(train_loader):  \n        correct = 0\n        total = 0\n        # Move tensors to the configured device\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        model.train()\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        \n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Loss {loss.item()}\" )\n    print('Accuracy of the network on the {}  images: {} %'.format(50000, 100 * correct / total))\n        \n    # Validation\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        eval_loss=0\n        for images, labels in tqdm(test_loader):\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            \n            eval_loss+=criterion(outputs, labels).item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            del images, labels, outputs\n        print(f\" Loss Minimum {minimum_loss} Loss_current {eval_loss}\")\n            \n        if eval_loss<=minimum_loss:\n            patience_count=0\n            minimum_loss=eval_loss\n            torch.save(model, 'Freezed.pt')\n        else:\n            patience_count+=1\n            print(f\"Patience Counter {patience_count}\")\n        print('Accuracy of the network on the {} validation images: {} %'.format(10000, 100 * correct / total))\n        \n        if patience_count>=patience:\n            print(f\"Early stopping initialized\")\n            break\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T17:47:08.184336Z","iopub.execute_input":"2024-07-11T17:47:08.184639Z","iopub.status.idle":"2024-07-11T17:55:30.200904Z","shell.execute_reply.started":"2024-07-11T17:47:08.184617Z","shell.execute_reply":"2024-07-11T17:55:30.200018Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"100%|██████████| 500/500 [07:42<00:00,  1.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss 1.8606089353561401\nAccuracy of the network on the 50000  images: 50.0 %\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 625/625 [00:38<00:00, 16.33it/s]\n","output_type":"stream"},{"name":"stdout","text":" Loss Minimum inf Loss_current 1143.2219876646996\nAccuracy of the network on the 10000 validation images: 49.22 %\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Passing Radnom image to find Cnn output","metadata":{}},{"cell_type":"code","source":"model = model.to(device)\na=torch.randn(1,3,224,224)\na=a.to(device)\n#center_crop = torchvision.transforms.CenterCrop(64)\n\n# Apply the center cropping transform to the input image\n#a = center_crop(a)\n\nx=model(a)\nx.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-11T17:55:30.201848Z","iopub.execute_input":"2024-07-11T17:55:30.202875Z","iopub.status.idle":"2024-07-11T17:55:30.262178Z","shell.execute_reply.started":"2024-07-11T17:55:30.202813Z","shell.execute_reply":"2024-07-11T17:55:30.261291Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 100])"},"metadata":{}}]}]}