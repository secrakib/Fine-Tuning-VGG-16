{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1782442,"sourceType":"datasetVersion","datasetId":1059701}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-12T08:01:14.444658Z","iopub.execute_input":"2024-04-12T08:01:14.445427Z","iopub.status.idle":"2024-04-12T08:01:14.882929Z","shell.execute_reply.started":"2024-04-12T08:01:14.445387Z","shell.execute_reply":"2024-04-12T08:01:14.881868Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/cifar100/meta\n/kaggle/input/cifar100/file.txt\n/kaggle/input/cifar100/test\n/kaggle/input/cifar100/train\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Transforming data","metadata":{}},{"cell_type":"code","source":"import torch \nimport torchvision\n\n\ntransform_train = torchvision.transforms.Compose([\n    torchvision.transforms.Resize(224),  # Resize to 256x256\n    #torchvision.transforms.CenterCrop(64),  # Crop a central 224x224 region\n    #torchvision.transforms.RandomResizedCrop(size=(64,64), scale=(0.8, 1.0)),  # Randomly resize and crop\n    torchvision.transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip horizontally\n    torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    torchvision.transforms.ToTensor(),  # Convert to PyTorch tensor\n    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for ImageNet\n])\n\ntransform_test=torchvision.transforms.Compose([\n    torchvision.transforms.Resize(224),  \n    torchvision.transforms.ToTensor(),  # Convert to PyTorch tensor\n    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2024-04-12T08:01:14.884941Z","iopub.execute_input":"2024-04-12T08:01:14.885366Z","iopub.status.idle":"2024-04-12T08:01:18.685005Z","shell.execute_reply.started":"2024-04-12T08:01:14.885339Z","shell.execute_reply":"2024-04-12T08:01:18.684022Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Importing and Loading Data ","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom torchvision.datasets import CIFAR100\n\nroot=\"./data\"\ntrain = CIFAR100(root=root, transform=transform_train,download=True,train=True)\ntest= CIFAR100(root=root, transform=transform_test,download=True,train=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T08:01:18.686376Z","iopub.execute_input":"2024-04-12T08:01:18.686840Z","iopub.status.idle":"2024-04-12T08:01:20.786068Z","shell.execute_reply.started":"2024-04-12T08:01:18.686810Z","shell.execute_reply":"2024-04-12T08:01:20.784855Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Initialize Dataloader","metadata":{}},{"cell_type":"code","source":"train_loader = DataLoader(train, batch_size=100, shuffle=True)\ntest_loader= DataLoader(test, batch_size=16, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T08:01:20.789074Z","iopub.execute_input":"2024-04-12T08:01:20.789494Z","iopub.status.idle":"2024-04-12T08:01:20.795544Z","shell.execute_reply.started":"2024-04-12T08:01:20.789441Z","shell.execute_reply":"2024-04-12T08:01:20.794490Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"len(train_loader)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T08:01:20.796908Z","iopub.execute_input":"2024-04-12T08:01:20.797872Z","iopub.status.idle":"2024-04-12T08:01:20.812247Z","shell.execute_reply.started":"2024-04-12T08:01:20.797833Z","shell.execute_reply":"2024-04-12T08:01:20.810893Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"500"},"metadata":{}}]},{"cell_type":"markdown","source":"# Define Architecture","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass VGG16(nn.Module):\n    def __init__(self, num_classes=10):\n        super(VGG16, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU())\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(), \n            nn.MaxPool2d(kernel_size = 2, stride = 2))\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU())\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2, stride = 2))\n        self.layer5 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU())\n        self.layer6 = nn.Sequential(\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU())\n        self.layer7 = nn.Sequential(\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2, stride = 2))\n        self.layer8 = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU())\n        self.layer9 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU())\n        self.layer10 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2, stride = 2))\n        self.layer11 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU())\n        self.layer12 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU())\n        self.layer13 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2, stride = 2))\n            \n        \n        self.fc = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(7*7*512, 4096),\n            nn.ReLU())\n        \n        self.fc1 = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU())\n        self.fc2= nn.Sequential(\n            nn.Linear(4096, num_classes))\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        #print(out.shape)\n        out = self.layer2(out)\n        #print(out.shape)\n        out = self.layer3(out)\n        #print(out.shape)\n        out = self.layer4(out)\n        #print(out.shape)\n        out = self.layer5(out)\n        #print(out.shape)\n        out = self.layer6(out)\n        #print(out.shape)\n        out = self.layer7(out)\n        #print(out.shape)\n        out = self.layer8(out)\n        #print(out.shape)\n        out = self.layer9(out)\n        #print(out.shape)\n        out = self.layer10(out)\n        #print(out.shape)\n        out = self.layer11(out)\n        #print(out.shape)\n        out = self.layer12(out)\n        #print(out.shape)\n        out = self.layer13(out)\n        #print(out.shape)\n    \n        out=out.flatten(start_dim=1)\n        #print(out.shape)\n        out = self.fc(out)\n        #print(out.shape)\n        \n        out = self.fc1(out)\n        #print(out.shape)\n        out = self.fc2(out)\n        #print(out.shape)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2024-04-12T08:01:20.813736Z","iopub.execute_input":"2024-04-12T08:01:20.814199Z","iopub.status.idle":"2024-04-12T08:01:20.847297Z","shell.execute_reply.started":"2024-04-12T08:01:20.814169Z","shell.execute_reply":"2024-04-12T08:01:20.845919Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"device='cuda'\nnum_classes = 100\nnum_epochs = 2\nlearning_rate = 0.001\nmodel = VGG16(num_classes)\n#model=nn.DataParallel(model)\nmodel=model.to(device)\n\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.7)  \n\n\n# Train the model\ntotal_step = len(train_loader)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T08:01:20.849211Z","iopub.execute_input":"2024-04-12T08:01:20.850042Z","iopub.status.idle":"2024-04-12T08:01:22.675872Z","shell.execute_reply.started":"2024-04-12T08:01:20.850004Z","shell.execute_reply":"2024-04-12T08:01:22.674824Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Traning and Testing loop","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\ntotal_step = len(train_loader)\npatience=2\npatience_count=0\nminimum_loss=float('inf')\n\nfor epoch in range(num_epochs):\n    \n    for  images, labels in tqdm(train_loader):  \n        correct = 0\n        total = 0\n        # Move tensors to the configured device\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        model.train()\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        \n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Loss {loss.item()}\" )\n    print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))\n        \n    # Validation\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        eval_loss=0\n        for images, labels in tqdm(test_loader):\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            \n            eval_loss+=criterion(outputs, labels).item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            del images, labels, outputs\n        print(f\" Loss Minimum {minimum_loss} Loss_current {eval_loss}\")\n            \n        if eval_loss<=minimum_loss:\n            patience_count=0\n            minimum_loss=eval_loss\n        else:\n            patience_count+=1\n            print(f\"Patience Counter {patience_count}\")\n        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))\n        \n        if patience_count>=patience:\n            print(f\"Early stopping initialized\")\n            break\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T08:01:22.677680Z","iopub.execute_input":"2024-04-12T08:01:22.678247Z","iopub.status.idle":"2024-04-12T08:21:18.648374Z","shell.execute_reply.started":"2024-04-12T08:01:22.678211Z","shell.execute_reply":"2024-04-12T08:21:18.647244Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|██████████| 500/500 [09:11<00:00,  1.10s/it]\n","output_type":"stream"},{"name":"stdout","text":"Loss 4.32022762298584\nAccuracy of the network on the 5000 validation images: 4.0 %\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 625/625 [00:44<00:00, 13.96it/s]\n","output_type":"stream"},{"name":"stdout","text":" Loss Minimum inf Loss_current 2711.293501853943\nAccuracy of the network on the 5000 validation images: 4.85 %\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 500/500 [09:13<00:00,  1.11s/it]\n","output_type":"stream"},{"name":"stdout","text":"Loss 3.8115530014038086\nAccuracy of the network on the 5000 validation images: 14.0 %\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 625/625 [00:44<00:00, 13.91it/s]","output_type":"stream"},{"name":"stdout","text":" Loss Minimum 2711.293501853943 Loss_current 2485.2630360126495\nAccuracy of the network on the 5000 validation images: 9.39 %\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Passing Radnom image to find Cnn output","metadata":{}},{"cell_type":"code","source":"model = VGG16(num_classes).to(device)\na=torch.randn(1,3,224,224)\na=a.to(device)\n#center_crop = torchvision.transforms.CenterCrop(64)\n\n# Apply the center cropping transform to the input image\n#a = center_crop(a)\n\nx=model(a)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T08:37:32.900896Z","iopub.execute_input":"2024-04-12T08:37:32.901797Z","iopub.status.idle":"2024-04-12T08:37:34.539613Z","shell.execute_reply.started":"2024-04-12T08:37:32.901765Z","shell.execute_reply":"2024-04-12T08:37:34.538679Z"},"trusted":true},"execution_count":14,"outputs":[]}]}